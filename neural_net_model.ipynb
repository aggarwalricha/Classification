{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.29977\u001b[0m\u001b[0m | time: 0.058s\n",
      "| Adam | epoch: 010 | loss: 0.29977 - acc: 0.8798 -- iter: 200/209\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.28263\u001b[0m\u001b[0m | time: 0.066s\n",
      "| Adam | epoch: 010 | loss: 0.28263 - acc: 0.8918 -- iter: 209/209\n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7222222089767456]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from contractions import CONTRACTION_MAP\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from enchant.checker import SpellChecker\n",
    "\n",
    "\n",
    "# create data corpus\n",
    "notes = [note for note in open(\"cleanNotes.txt\")]\n",
    "\n",
    "labels = [int(re.sub(\"\\n\",'',label)) for label in open(\"labels.txt\")]\n",
    "labels = to_categorical(labels, 2)\n",
    "\n",
    "\n",
    "\n",
    "def stemWord(note):\n",
    "    tokens = tokenize_text(note)\n",
    "    new_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for word in tokens:\n",
    "        #new_words.append(word)\n",
    "        new_word = stemmer.stem(word)\n",
    "        new_words.append(new_word)\n",
    "    \n",
    "    return \" \".join([word for word in new_words])\n",
    "    \n",
    "def lemmatize(words):\n",
    "    tokens = tokenize_text(note)\n",
    "    new_words = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in tokens:\n",
    "        new_word = lemmatizer.lemmatize(word, pos='v')\n",
    "        new_words.append(new_word)\n",
    "    \n",
    "    return \" \".join([word for word in new_words])\n",
    "\n",
    "# normalize corpus\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower()) \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "    \n",
    "def tokenize_text(text):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "def remove_special_chars(note):\n",
    "    tokens = tokenize_text(note)\n",
    "    filtered_Words =[]\n",
    "    for token in tokens:\n",
    "        new_word = ''.join([ch for ch in token if ch not in string.punctuation])\n",
    "        filtered_Words.append(new_word)\n",
    "    \n",
    "    return \" \".join([word for word in filtered_Words])\n",
    "        \n",
    "def remove_stopwords(note):\n",
    "    tokens = tokenize_text(note)\n",
    "    filtered = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join([tk for tk in filtered])\n",
    "    \n",
    "def correct_spell(text):\n",
    "    chkr = SpellChecker(\"en-US\",\"en-UK\")\n",
    "    chkr.set_text(text)\n",
    "    for err in chkr:\n",
    "        #print err \n",
    "        if len(err.suggest()) > 0:\n",
    "            sug = err.suggest()[0]\n",
    "            #print \"suggest: \" + sug\n",
    "            err.replace(sug)\n",
    "    \n",
    "    correctedNote = chkr.get_text()\n",
    "    \n",
    "    return correctedNote \n",
    "\n",
    "def normalize(notes):\n",
    "    normalized_notes =[]\n",
    "    \n",
    "    for note in notes:\n",
    "        note = expand_contractions(note, CONTRACTION_MAP) # To write yourself\n",
    "        #note = correct_spell(note)\n",
    "        note = note.lower()\n",
    "        note = remove_special_chars(note)\n",
    "        note = remove_stopwords(note)\n",
    "        #note = stemWord(note)\n",
    "        #note = lemmatize(note)\n",
    "        \n",
    "        # more methods to add\n",
    "        normalized_notes.append(note)\n",
    "    \n",
    "    return normalized_notes\n",
    "    \n",
    "\n",
    "normalized_notes = normalize(notes)\n",
    "#print len(normalized_notes)\n",
    "#print len(labels)\n",
    "\n",
    "# build feature matrix\n",
    "vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1))\n",
    "feature_matrix = vectorizer.fit_transform(normalized_notes).astype(float).toarray()\n",
    "\n",
    "vocab_size = len(vectorizer.vocabulary_)\n",
    "\n",
    "# split data into 70:30\n",
    "x_train , x_test , y_train , y_test = train_test_split(feature_matrix, labels, test_size=0.3, random_state=21, stratify=labels)\n",
    "\n",
    "# set up neural network\n",
    "tf.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, vocab_size])\n",
    "net = tflearn.fully_connected(net , 30)\n",
    "net = tflearn.fully_connected(net , 30)\n",
    "net = tflearn.fully_connected(net , 2 , activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# train\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(x_train, y_train, n_epoch=10, batch_size=25, show_metric=True)\n",
    "\n",
    "#test\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
