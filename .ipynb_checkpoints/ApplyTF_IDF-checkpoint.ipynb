{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "#Read data from existing CSV file and write into a text file \n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "dataframe = pd.read_csv(\"data.csv\", header =0, usecols=[\"Outcome\", \"Salesforce Notes\"])\n",
    "dataframe.to_csv(\"notesOutput.txt\", \n",
    "                 columns=[\"Outcome\", \"Salesforce Notes\"], \n",
    "                 header=[\"Autoclose\", \"Notes\"], \n",
    "                 sep=' ',\n",
    "                 index=None)\n",
    "\n",
    "\n",
    "\"\"\"-----------------------------------------------------------------\"\"\"\n",
    "#Apply TDF\n",
    "\n",
    "start = time.time()\n",
    "#.1 Extract documents (notes) from notesOutput.txt file\n",
    "dataframe = pd.read_csv(\"notesOutput.txt\", header=None, sep=' ', skiprows=[0])\n",
    "documents = dataframe[dataframe.columns[1:2]]\n",
    "\n",
    "#.2 Extract set of unique tokens from all the documents and prepare dict containing set of unique words and all words\n",
    "\n",
    "set_of_unique_tokens = []\n",
    "corpus_dict ={}\n",
    "\n",
    "def getAllWords(document):\n",
    "    \"\"\"\n",
    "    all_words =[]\n",
    "    text = document[1]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        word_token = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(word_token)\n",
    "        all_words.append(tagged)\n",
    "    \"\"\"\n",
    "    all_words = str(document[1]).split(' ')\n",
    "    return all_words\n",
    "    \n",
    "for index ,document in enumerate(documents.itertuples()):\n",
    "    all_words = getAllWords(document)\n",
    "    #print all_words\n",
    "\n",
    "for index ,document in enumerate(documents.itertuples()):\n",
    "    all_words = getAllWords(document)\n",
    "    unique_words = set(all_words)\n",
    "    doc_details ={'unique_words': unique_words, 'all_words': all_words}\n",
    "    corpus_dict[index] = doc_details\n",
    "\n",
    "x = [(val['unique_words'])for val in corpus_dict.values()]\n",
    "set_of_unique_tokens = set.union(*x)\n",
    "\n",
    "#print len(set_of_unique_tokens)\n",
    "            \n",
    "def calculateIDF(token):\n",
    "    total_docs = len(documents)\n",
    "    no_of_docs_having_token =0\n",
    "    for val in corpus_dict.values():\n",
    "        if token in val['unique_words']:\n",
    "            no_of_docs_having_token+=1\n",
    "    idf = math.log(total_docs / float(no_of_docs_having_token))\n",
    "    #print('index: %d token: %s   no_of_docs: %s   idf:%s'% (index, token , no_of_docs_having_token, idf))\n",
    "    return idf\n",
    "\n",
    "def calculateTF(token , doc_words):\n",
    "    word_freq = doc_words.count(token) \n",
    "    total_words = len(doc_words)\n",
    "    tf = word_freq / float(total_words)\n",
    "    return tf\n",
    "\n",
    "\n",
    "\n",
    "for token in set_of_unique_tokens:\n",
    "    idf_token = calculateIDF(token)\n",
    "    \n",
    "    for val in corpus_dict.values():\n",
    "        tf_token = calculateTF(token , val['all_words'])\n",
    "        tf_idf =  tf_token * idf_token\n",
    "        print(\"tf_idf:%s  token:%s \"%(tf_idf, token))\n",
    "\n",
    "end = time.time()\n",
    "print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
