{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Test Loss: 0.67 , Test Acc: 0.66667\n",
      "Epoch:2, Test Loss: 0.72 , Test Acc: 0.66667\n",
      "Epoch:3, Test Loss: 0.77 , Test Acc: 0.68333\n",
      "Epoch:4, Test Loss: 0.89 , Test Acc: 0.73333\n",
      "Epoch:5, Test Loss: 0.77 , Test Acc: 0.66667\n",
      "Epoch:6, Test Loss: 0.98 , Test Acc: 0.7\n",
      "Epoch:7, Test Loss: 1.3 , Test Acc: 0.68333\n",
      "Epoch:8, Test Loss: 1.7 , Test Acc: 0.66667\n",
      "Epoch:9, Test Loss: 3.9 , Test Acc: 0.68333\n",
      "Epoch:10, Test Loss: 0.83 , Test Acc: 0.68333\n",
      "Epoch:11, Test Loss: 1.1 , Test Acc: 0.68333\n",
      "Epoch:12, Test Loss: 1.6 , Test Acc: 0.66667\n",
      "Epoch:13, Test Loss: 1.8 , Test Acc: 0.7\n",
      "Epoch:14, Test Loss: 1.9 , Test Acc: 0.7\n",
      "Epoch:15, Test Loss: 2.0 , Test Acc: 0.7\n",
      "Epoch:16, Test Loss: 2.0 , Test Acc: 0.7\n",
      "Epoch:17, Test Loss: 2.1 , Test Acc: 0.7\n",
      "Epoch:18, Test Loss: 2.1 , Test Acc: 0.7\n",
      "Epoch:19, Test Loss: 2.1 , Test Acc: 0.7\n",
      "Epoch:20, Test Loss: 2.1 , Test Acc: 0.71667\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reading Text file , extracting Notes content\n",
    "def getProcessedText(data):\n",
    "    return str(data).decode('ascii','ignore').encode('ascii','ignore')\n",
    "\n",
    "dataframe = pd.read_csv(\"notesOutput.txt\", sep=' ',index_col=None)\n",
    "data = [getProcessedText(doc) for doc in dataframe[\"Notes\"]]\n",
    "labels = map(lambda label: 1 if label.startswith('Pos') else 0, dataframe[\"Autoclose\"])\n",
    "\n",
    "# average length of a review \n",
    "max_seq_length = sum([len(note.split(\" \")) for note in data]) / len(data)\n",
    "\n",
    "#mapping every word to unique Id\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_seq_length)\n",
    "x_data = np.array(list(vocab_processor.fit_transform(data))) # every word to its corresponding index in unique word list\n",
    "y_output = np.array(labels)\n",
    "\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "\n",
    "# Divinding data into Train-test data in 70:30\n",
    "train_data, test_data, train_target , test_target = train_test_split(x_data , y_output , test_size=0.3, random_state=21, stratify=y_output)\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None , max_seq_length])\n",
    "y = tf.placeholder(tf.int32,[None])\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 25\n",
    "embedding_size = 50 # vector size of one word\n",
    "max_label =2\n",
    "\n",
    "# vector of all vocab words squishing to 50 on scale of -1.0 to 1.0\n",
    "embedding_matrix = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embeddings = tf.nn.embedding_lookup(embedding_matrix, x)\n",
    "embeddings\n",
    "\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(embedding_size) # no of neurons in each cell\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell , output_keep_prob=0.75)\n",
    "\n",
    "output , (final_state_memory_cell , otherInfo) = tf.nn.dynamic_rnn(lstmCell , embeddings , dtype=tf.float32)\n",
    "\n",
    "#softmax layer\n",
    "logits = tf.layers.dense(final_state_memory_cell, max_label, activation=None)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits , labels=y)\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction , tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        num_batches = int(len(train_data) // batch_size) + 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            min_ix = i* batch_size\n",
    "            max_ix = np.min([len(train_data), ((i+1)*batch_size)])\n",
    "            \n",
    "            x_train_batch = train_data[min_ix: max_ix]\n",
    "            y_train_batch = train_target[min_ix: max_ix]\n",
    "            \n",
    "            train_dict = {x: x_train_batch , y: y_train_batch}\n",
    "            session.run(train_step, feed_dict = train_dict)\n",
    "\n",
    "            train_loss , train_acc = session.run([loss, accuracy], feed_dict=train_dict)\n",
    "            \n",
    "        test_dict = {x: test_data , y:test_target}\n",
    "        test_loss, test_acc = session.run([loss , accuracy], feed_dict = test_dict)\n",
    "        print('Epoch:{}, Test Loss: {:.2} , Test Acc: {:.5}'.format(epoch+1, test_loss , test_acc))\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
